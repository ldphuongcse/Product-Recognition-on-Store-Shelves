{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step B - Multiple Instance Detection\n",
    "\n",
    "- Baraghini Nicholas\n",
    "- Marini Luca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sift = cv2.xfeatures2d.SIFT_create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Matching(Model_Descriptors, Scene_Descriptors, Treshold = 0.45, k=2):\n",
    "    FLANN_INDEX_KDTREE = 1\n",
    "\n",
    "        # Defining parameters for algorithm \n",
    "    index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "\n",
    "        # Defining search params.\n",
    "        # checks=50 specifies the number of times the trees in the index should be recursively traversed.\n",
    "        # Higher values gives better precision, but also takes more time\n",
    "    search_params = dict(checks = 50)\n",
    "\n",
    "        # Initializing matcher\n",
    "    flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "        # Matching and finding the 2 closest elements for each query descriptor.\n",
    "    matches = flann.knnMatch(Model_Descriptors, Scene_Descriptors, k)\n",
    "        #defining an array containing all the matches that results to be considered \"good\" matches applying a certain treshold \n",
    "    good = []\n",
    "    for m,n in matches:\n",
    "        if m.distance < Treshold * n.distance: #  if m.distance/n.distance < Threshold:\n",
    "            good.append(m)\n",
    "            \n",
    "    return good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definition of the function which find the model center and stores it in the dictionary \"model_images_features\"\n",
    "def InstanceCenter(model_img,model_images_features, i, CENTER_INDEX=3):\n",
    "    height, width, channels = model_img.shape\n",
    "    v = int(height/2) # vertical coordinate of the image center\n",
    "    h = int(width/2)  # horizontal position of the image center\n",
    "    G = np.array([h, v]) # Defnition of the position center of the model\n",
    "    if len( model_images_features[i])<=CENTER_INDEX:\n",
    "        model_images_features[i].append(G) # Updating the model feature dictionary appending the information of the center\n",
    "    else :\n",
    "        #in the case G is already present in the model feature dictionary then update the G alreary there\n",
    "        model_images_features[i][CENTER_INDEX] = G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_OF_MODELS = 27\n",
    "\n",
    "IMAGE_INDEX      = 0\n",
    "KEYPOINT_INDEX   = 1\n",
    "DESCRIPTOR_INDEX = 2\n",
    "CENTER_INDEX     = 3\n",
    "V_INDEX          = 4\n",
    "TRESHOLD_INDEX = 5\n",
    "\n",
    "# Dictionary that contains the image, all keypoints and descriptors for each model images\n",
    "model_images_features = {}\n",
    "mean_of_model_intensities_r_g_b = {}\n",
    "\n",
    "for i in range(NUM_OF_MODELS):\n",
    "    model_img = cv2.imread('./models/{}.jpg'.format(i), cv2.COLOR_BGR2RGB)\n",
    "    kp_model = sift.detect(model_img)\n",
    "    kp_model, des_model = sift.compute(model_img, kp_model)\n",
    "    model_images_features[i] = [model_img, kp_model, des_model]\n",
    "    \n",
    "    b,g,r = cv2.split(model_images_features[i][IMAGE_INDEX])\n",
    "    # save the mean of the intensities (divided per channel) for every model image\n",
    "    mean_of_model_intensities_r_g_b[i] = [np.mean(r), np.mean(g), np.mean(b)]\n",
    "    #Find or update the center of the model image information collected in the model features dictionary\n",
    "    InstanceCenter(model_img,model_images_features, i, CENTER_INDEX=3) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_2_points(A, B):\n",
    "    return math.sqrt( np.power(A[0] - B[0], 2) +  np.power(A[1] - B[1], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenes_to_test = [1, 2, 3, 4, 5]\n",
    "\n",
    "# Dictionary that contains the image, all keypoints and descriptors for each easy scene image\n",
    "m_scenes_images_features = {}\n",
    "\n",
    "for i in scenes_to_test:\n",
    "    scene_img = cv2.imread('./scenes/m{}.png'.format(i), cv2.COLOR_BGR2RGB)\n",
    "    kp_scene = sift.detect(scene_img)\n",
    "    kp_scene, des_scene = sift.compute(scene_img, kp_scene)\n",
    "    m_scenes_images_features[i] = [scene_img, kp_scene, des_scene]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function create or update the model_images_features dictionary which contains all the relevant features of the \n",
    "# each model image, with a list containing all the voting vectors computed for each good match between the keypoints of \n",
    "# the model and the scene image\n",
    "\n",
    "\n",
    "def VotingVectors(Good_Matches,model_images_features, i):\n",
    "    # getting the coordinates of the center of the i-th model, G\n",
    "    G_x = model_images_features[i][CENTER_INDEX][0]\n",
    "    G_y = model_images_features[i][CENTER_INDEX][1]\n",
    "        \n",
    "    #Initializing the vector V \n",
    "    V = []\n",
    "    \n",
    "    for m in Good_Matches:\n",
    "        # getting the coordinates of the m-th keypoint in the i-th model\n",
    "        Kp_x = int(model_images_features[i][KEYPOINT_INDEX][m.queryIdx].pt[0])\n",
    "        Kp_y = int(model_images_features[i][KEYPOINT_INDEX][m.queryIdx].pt[1])\n",
    "            \n",
    "        #defining the point V = Kp[m,i] - G[i] \n",
    "        vx = Kp_x - G_x\n",
    "        vy = Kp_y - G_y\n",
    "            \n",
    "        #computing the slope of the direction aligning the center point G with the m-th keypoint\n",
    "        #in the case V = (0,0) it means basically that the point G[i] and Kp[m,i] are coincident and so the slope will not exist;\n",
    "        # Or anothe case where the slope does not exist is if the the G[i] and the Kp[m,i] are aligned in a vertical line;\n",
    "        #In such a cases the slope variable can be considered as an array containing two informations, the fisrt information is the \n",
    "        #actual value of the slope, while the second information encloses if the slope exists or not (1,0 respectively)\n",
    "        if (np.abs(vx) + np.abs(vy)) == 0 :\n",
    "            slope = [0, 0]\n",
    "        elif vy == 0 :\n",
    "            slope = [1, 0]\n",
    "        else:\n",
    "            slope = [vx/vy, 1]\n",
    "            \n",
    "        #creating an array containing the V coordinates and the slope of the line \n",
    "        v = [vx, vy, slope]\n",
    "        V.append(v)\n",
    "\n",
    "    if len( model_images_features[i]) <= V_INDEX :\n",
    "        # Updating the model feature dictionary appending the information of the vector V\n",
    "        model_images_features[i].append(V) \n",
    "    else: \n",
    "        #in the case V is already present in the model feature dictionary then update the V alreary there\n",
    "        model_images_features[i][V_INDEX] = V\n",
    "\n",
    "    return V    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KeyPoints_Comparison(models_images_features,scenes_images_features, Good_Matches):\n",
    "    model_kps_size = [] #list containing the size of each matched keypoint belonging to the model\n",
    "    scene_kps_size = [] #list containing the size of each matched keypoint belonging to the scene\n",
    "    ratio_of_sizes = [] #list containing the ratio between the sizes of the same keypoint found in the model and in the scene, representing the change in dimensions\n",
    "    \n",
    "    model_kps_angle = [] #list containing the angle of each matched keypoint belonging to the model\n",
    "    scene_kps_angle = [] #list containing the angle of each matched keypoint belonging to the scene\n",
    "    relative_angles = [] #list containing the angle between the model and the scene, representing rotation occuring between model and scene instance\n",
    "    \n",
    "    for o in Good_Matches:\n",
    "             #Defining two vectors respectively containing the size of the keypoints belonging to the model and \n",
    "             #to the scene\n",
    "        M_kp_size = np.float32(models_images_features[i][KEYPOINT_INDEX][o.queryIdx].size)\n",
    "        S_kp_size = np.float32(scenes_images_features[j][KEYPOINT_INDEX][o.trainIdx].size)\n",
    "        model_kps_size.append(M_kp_size)\n",
    "        scene_kps_size.append(S_kp_size)\n",
    "             #Defining two vectors respectively containing the angle of the keypoints belonging to the model and \n",
    "             #to the scene        \n",
    "        M_kp_angle = np.float32(models_images_features[i][KEYPOINT_INDEX][o.queryIdx].angle)\n",
    "        S_kp_angle = np.float32(scenes_images_features[j][KEYPOINT_INDEX][o.trainIdx].angle)\n",
    "        model_kps_angle.append(M_kp_angle)\n",
    "        scene_kps_angle.append(S_kp_angle)\n",
    "\n",
    "             #defining the change in dimensions between the keypoints of the model compared to the same ones found \n",
    "             #in the scene side\n",
    "        ratio_of_sizes.append(M_kp_size/S_kp_size)\n",
    "        \n",
    "             #defining the change in dimensions between the keypoints of the model compared to the same ones found \n",
    "             #in the scene side\n",
    "        relative_angles.append(M_kp_angle-S_kp_angle)       \n",
    "\n",
    "    #computation of the mean scale factor\n",
    "    if len(ratio_of_sizes) : \n",
    "        Mean_Scale_Factor = (sum(ratio_of_sizes))/len(ratio_of_sizes)\n",
    "    else: \n",
    "        # Mean_Scale_Factor = 0\n",
    "        Mean_Scale_Factor = 3.5\n",
    "    #computation of the mean relative angle\n",
    "    if len(relative_angles) :\n",
    "        Mean_Relative_Angle = (sum(relative_angles))/len(relative_angles)\n",
    "    else: \n",
    "        Mean_Relative_Angle = 0\n",
    "        \n",
    "        \n",
    "    return Mean_Scale_Factor, Mean_Relative_Angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Accumulator_Array(ACC_ARRAY_CELL_DIMENSION_1, ACC_ARRAY_CELL_DIMENSION_2, Scene_img, Model_img, \n",
    "                      models_images_features, scenes_images_features, Good_Matches, V, i, j, KEYPOINT_INDEX = 1):\n",
    "    #Accumulator dimensions\n",
    "    Acc_dim = (int(scenes_images_features[j][0].shape[0] / ACC_ARRAY_CELL_DIMENSION_2), \n",
    "               int(scenes_images_features[j][0].shape[1] / ACC_ARRAY_CELL_DIMENSION_1))\n",
    "    Accumulator_Array_Points = {}\n",
    "    #Accumulator array as a matrix of zeroes\n",
    "    Accumulator_Array = np.zeros(Acc_dim)\n",
    "    #print(Accumulator_Array.shape)\n",
    "    #extracting all the keypoints of the scene resulting in good matches\n",
    "    scene_pts = np.float32([scenes_images_features[j][KEYPOINT_INDEX][m.trainIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "    \n",
    "    #computing the mean scale factor and the mean relative angle occuring between the model and its instances on the scene\n",
    "    r , alpha = KeyPoints_Comparison(models_images_features, scenes_images_features, Good_Matches)\n",
    "    \n",
    "    #defining the list wich will contain all the G estimation\n",
    "    G_scene = []\n",
    "    for l in range(len(Good_Matches)):\n",
    "        # print((models_images_features[i][KEYPOINT_INDEX][Good_Matches[l].queryIdx].pt))\n",
    "        #rescaling of the voting vectors\n",
    "        Vx_scene = V[l][0] / r\n",
    "        Vy_scene = V[l][1] / r\n",
    "        #computing the estimate position of the center pointed by the l-th voting vector rescaled starting from the l-th keypoint\n",
    "        Gx_scene = scene_pts[l][0][0] - Vx_scene\n",
    "        Gy_scene = scene_pts[l][0][1] - Vy_scene\n",
    "\n",
    "        #collecting the Center estimation in a list \n",
    "        G_scene.append([Gx_scene, Gy_scene])\n",
    "        \n",
    "        \n",
    "        GS_x = int(Gx_scene/(ACC_ARRAY_CELL_DIMENSION_1))\n",
    "        GS_y = int(Gy_scene/(ACC_ARRAY_CELL_DIMENSION_2))\n",
    "        # print([Gx_scene, Gy_scene])\n",
    "        if GS_x in range(Acc_dim[1]):\n",
    "            if GS_y in range(Acc_dim[0]):\n",
    "                # print([GS_y, GS_x])\n",
    "                Accumulator_Array[GS_y,GS_x] += 1\n",
    "\n",
    "                # save the scene points that fall into the current cell of the accumulator array, \n",
    "                if not (GS_y,GS_x, 'S') in Accumulator_Array_Points:\n",
    "                    Accumulator_Array_Points[(GS_y, GS_x, 'S')] = []\n",
    "                    Accumulator_Array_Points[(GS_y, GS_x, 'S')].append((Gx_scene, Gy_scene))\n",
    "                else:\n",
    "                    Accumulator_Array_Points[(GS_y, GS_x, 'S')].append((Gx_scene, Gy_scene))\n",
    "\n",
    "    return Accumulator_Array, G_scene, Accumulator_Array_Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_estimation(models_images_features, scenes_images_features, Good_Matches, V, j):\n",
    "    #extracting all the keypoints of the scene resulting in good matches\n",
    "    scene_pts = np.float32([scenes_images_features[j][KEYPOINT_INDEX][m.trainIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "    \n",
    "    #computing the mean scale factor and the mean relative angle occuring between the model and its instances on the scene\n",
    "    r , alpha = KeyPoints_Comparison(models_images_features, scenes_images_features, Good_Matches)\n",
    "    \n",
    "    #defining the list wich will contain all the G estimation\n",
    "    G_scene = []\n",
    "    for l in range(len(Good_Matches)):\n",
    "        # print((models_images_features[i][KEYPOINT_INDEX][Good_Matches[l].queryIdx].pt))\n",
    "        #rescaling of the voting vectors\n",
    "        Vx_scene = V[l][0] / r\n",
    "        Vy_scene = V[l][1] / r\n",
    "        #computing the estimate position of the center pointed by the l-th voting vector rescaled starting from the l-th keypoint\n",
    "        Gx_scene = scene_pts[l][0][0] - Vx_scene\n",
    "        Gy_scene = scene_pts[l][0][1] - Vy_scene\n",
    "\n",
    "        #collecting the Center estimation in a list \n",
    "        G_scene.append([Gx_scene, Gy_scene])\n",
    "    \n",
    "    return G_scene, r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that solves exceeding dimensions of bounding_boxes in scene\n",
    "def solve_exceeding_dimensions_of_bounding_boxes_in_scene(final_corners_of_bounding_boxes, difficulty_scenes_images_features):\n",
    "    final_corners_of_bounding_boxes_without_exceeding_dimensions = []\n",
    "    # adjust bounding boxes that go out the dimensions of the scene image\n",
    "    scene_height = difficulty_scenes_images_features[j][IMAGE_INDEX].shape[0]\n",
    "    scene_width = difficulty_scenes_images_features[j][IMAGE_INDEX].shape[1]\n",
    "\n",
    "    for index, [fin_top_left_corner, fin_bottom_right_corner] in enumerate(final_corners_of_bounding_boxes):\n",
    "        fin_top_left_corner = list(fin_top_left_corner)\n",
    "        fin_bottom_right_corner = list(fin_bottom_right_corner)\n",
    "        # top left corner that goes out on the left of the scene image\n",
    "        if fin_top_left_corner[0] < 0:\n",
    "            fin_top_left_corner[0] = 0\n",
    "        # bottom right corner that goes out on the left of the scene image\n",
    "        if fin_bottom_right_corner[0] < 0:\n",
    "            fin_bottom_right_corner[0] = 0\n",
    "        # top left corner that goes out on the right of the scene image\n",
    "        if fin_top_left_corner[0] > scene_width:\n",
    "            fin_top_left_corner[0] = scene_width\n",
    "        # bottom right corner that goes out on the right of the scene image\n",
    "        if fin_bottom_right_corner[0] > scene_width:\n",
    "            fin_bottom_right_corner[0] = scene_width\n",
    "        # top left corner that goes out on the top of the scene image\n",
    "        if fin_top_left_corner[1] < 0:\n",
    "            fin_top_left_corner[1] = 0\n",
    "        # bottom right corner that goes out on the top of the scene image\n",
    "        if fin_bottom_right_corner[1] < 0:\n",
    "            fin_bottom_right_corner[1] = 0  \n",
    "        # top left corner that goes out on the bottom of the scene image\n",
    "        if fin_top_left_corner[1] > scene_height:\n",
    "            fin_top_left_corner[1] = scene_height\n",
    "        # bottom right corner that goes out on the bottom of the scene image\n",
    "        if fin_bottom_right_corner[1] > scene_height:\n",
    "            fin_bottom_right_corner[1] = scene_height\n",
    "\n",
    "        fin_top_left_corner = tuple(fin_top_left_corner)\n",
    "        fin_bottom_right_corner = tuple(fin_bottom_right_corner)\n",
    "        final_corners_of_bounding_boxes_without_exceeding_dimensions.append([ (int(fin_top_left_corner[0]), \n",
    "                                                                               int(fin_top_left_corner[1])), \n",
    "                                                                             (int(fin_bottom_right_corner[0]), \n",
    "                                                                              int(fin_bottom_right_corner[1]))])\n",
    "    return final_corners_of_bounding_boxes_without_exceeding_dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that merges adjacent bounding boxes that overlap between each other       \n",
    "def merge_overlapping_bounding_boxes(corners_of_bounding_boxes, DISTANCE_BOUNDING_BOXES_TRESHOLD=200):\n",
    "    # top left and bottom right corners of the final bounding boxes\n",
    "    final_corners_of_bounding_boxes = []\n",
    "    \n",
    "    for index_1, [top_left_corner, bottom_right_corner] in enumerate(corners_of_bounding_boxes):\n",
    "            # add first couple of top left and bottom right corners of the final bounding boxes\n",
    "            if len(final_corners_of_bounding_boxes) == 0:\n",
    "                final_corners_of_bounding_boxes.append([ (int(top_left_corner[0]), int(top_left_corner[1])), \n",
    "                                                            (int(bottom_right_corner[0]), int(bottom_right_corner[1]))])\n",
    "            \n",
    "            \n",
    "            for index_2, [fin_top_left_corner, fin_bottom_right_corner] in enumerate(final_corners_of_bounding_boxes):\n",
    "                # if a corner is already in the final corners then I don't add it\n",
    "                if top_left_corner == fin_top_left_corner and bottom_right_corner == fin_bottom_right_corner:\n",
    "                    break\n",
    "                # if a corner is near (below DISTANCE_BOUNDING_BOXES_TRESHOLD) a final corner, then I mean the two into a\n",
    "                # single one\n",
    "                if ( distance_2_points(top_left_corner, fin_top_left_corner) < DISTANCE_BOUNDING_BOXES_TRESHOLD and \n",
    "                distance_2_points(fin_bottom_right_corner, bottom_right_corner) < DISTANCE_BOUNDING_BOXES_TRESHOLD ) :\n",
    "                    \n",
    "                    sum_top_left = tuple(map(operator.add, top_left_corner, fin_top_left_corner))\n",
    "                    sum_bottom_right = tuple(map(operator.add, bottom_right_corner ,fin_bottom_right_corner))\n",
    "                    mean_top_left = (sum_top_left[0]/2, sum_top_left[1]/2)\n",
    "                    mean_bottom_right = (sum_bottom_right[0]/2, sum_bottom_right[1]/2)\n",
    "                    #print('mean_top_left', mean_top_left)\n",
    "                    final_corners_of_bounding_boxes[index_2] = [ (int(mean_top_left[0]), int(mean_top_left[1])), \n",
    "                                                            (int(mean_bottom_right[0]), int(mean_bottom_right[1]))]\n",
    "        \n",
    "                    break\n",
    "                # if my corner is'n near any final corner then I add it to the final corners\n",
    "                if index_2 == len(final_corners_of_bounding_boxes) - 1:\n",
    "                    final_corners_of_bounding_boxes.append([ (int(top_left_corner[0]), int(top_left_corner[1])), \n",
    "                                                            (int(bottom_right_corner[0]), int(bottom_right_corner[1]))])\n",
    "    return final_corners_of_bounding_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that split an image into (n_bins_heigth x n_bins_width) bins and returns a dictonary\n",
    "# that contains the means of the 3 color channels of the (n_bins_heigth x n_bins_width) bins\n",
    "def split_image_into_N_x_M_bins_with_intensity_means(image, n_bins_width = 3, n_bins_heigth = 4):\n",
    "    img_N_x_M_bins = {}\n",
    "    \n",
    "    img2 = np.copy(image)\n",
    "    \n",
    "    img_width = image.shape[1]\n",
    "    img_height = image.shape[0]\n",
    "    \n",
    "    step_width = int(img_width / n_bins_width)\n",
    "    step_height = int(img_height / n_bins_heigth)\n",
    "    \n",
    "    r = 0\n",
    "    c = 0\n",
    "    \n",
    "    #print('img_height: ', img_height)\n",
    "    #print('step_height: ', step_height)\n",
    "    if img_height != 0 and step_height != 0 and img_width != 0 and step_width != 0:\n",
    "        for row in np.arange(0, img_height, step_height):\n",
    "            c = 0\n",
    "            cv2.line(img2,(0, row),(img_width, row),(0,0,0),3) \n",
    "            for col in np.arange(0, img_width, step_width):\n",
    "                # print('row {} col {}'.format(r,c))\n",
    "                if row + 2 * step_height > img_height and col + 2 * step_width > img_width:\n",
    "                    partial_r_channel, partial_g_channel, partial_b_channel = cv2.split(image[row:, col:])\n",
    "\n",
    "                elif row + 2 * step_height > img_height:\n",
    "                    partial_r_channel, partial_g_channel, partial_b_channel = cv2.split(image[row:, col : col + step_width])\n",
    "\n",
    "                elif col + 2 * step_width > img_width:\n",
    "                    partial_r_channel, partial_g_channel, partial_b_channel = cv2.split(image[row : row + step_height, col :])\n",
    "                else:\n",
    "                    partial_r_channel, partial_g_channel, partial_b_channel = cv2.split(\n",
    "                        image[row : row + step_height, col : col + step_width])\n",
    "\n",
    "                cv2.line(img2,(col, 0),(col, img_height),(0,0,0),3) \n",
    "                if r < n_bins_heigth and c < n_bins_width:\n",
    "                    # save means of the 3 channels (r,g,b) of each bin\n",
    "                    img_N_x_M_bins[r, c] = (np.mean(partial_r_channel), np.mean(partial_g_channel), np.mean(partial_b_channel))\n",
    "                    # IF YOU WANT TO VISUALIZE EACH BIN UNCOMMENT THE FOLLOWING LINES:\n",
    "                    # plot each bin in blue channel color\n",
    "                    # plt.imshow(cv2.cvtColor(partial_b_channel, cv2.COLOR_BGR2RGB))\n",
    "                    # plt.show()\n",
    "\n",
    "                    # salva le medie e non le immagini dei bins\n",
    "\n",
    "                c += 1\n",
    "            r += 1\n",
    "        #plt.imshow(cv2.cvtColor(img2, cv2.COLOR_BGR2RGB))\n",
    "        #plt.show()  \n",
    "    return img_N_x_M_bins\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_color_problem_with_N_x_M_bins(final_corners_of_bounding_boxes_without_exceeding_dimensions, \n",
    "                                    difficulty_scenes_images_features, i, j, N=3, M=4, \n",
    "                                    COLOR_DIFF_IN_SINGLE_CHANNEL_TRES=50):\n",
    "    final_corners_of_bounding_boxes_after_color_problem = []\n",
    "    for index, [fin_top_left_corner, fin_bottom_right_corner] in enumerate(final_corners_of_bounding_boxes_without_exceeding_dimensions):\n",
    "    \n",
    "        means_bins_model_img = split_image_into_N_x_M_bins_with_intensity_means(\n",
    "            model_images_features[i][IMAGE_INDEX], \n",
    "            N, M)\n",
    "        \n",
    "        #print(fin_top_left_corner[1], fin_top_left_corner[0], fin_bottom_right_corner[1], fin_bottom_right_corner[0])\n",
    "        \n",
    "        means_bins_scene_img = split_image_into_N_x_M_bins_with_intensity_means(\n",
    "            difficulty_scenes_images_features[j][IMAGE_INDEX][fin_top_left_corner[1]:fin_bottom_right_corner[1], \n",
    "                                                                   fin_top_left_corner[0]:fin_bottom_right_corner[0]], \n",
    "            N, M)\n",
    "    \n",
    "        if not means_bins_scene_img or not means_bins_scene_img:\n",
    "            return []\n",
    "    \n",
    "        good = True\n",
    "        \n",
    "        # cycle the model dictionary and get the diff of the means by getting the keys of the scene dictionary\n",
    "        for k, v in means_bins_model_img.items():\n",
    "            means_k_scene = means_bins_scene_img.get(k)\n",
    "            # I consider a bounding box as good if and only if all the difference of the intiensities from the \n",
    "            # bins of the model, in all 3 channels, are below a certain treshold\n",
    "            diff_r = np.absolute(v[0] - means_k_scene[0])\n",
    "            diff_g = np.absolute(v[1] - means_k_scene[1])\n",
    "            diff_b = np.absolute(v[2] - means_k_scene[2])\n",
    "            \n",
    "            #print('Bin ({}, {}):'.format(k[0], k[1]))\n",
    "            # print('  - diff_r: ', diff_r)\n",
    "            # print('  - diff_g: ', diff_g)\n",
    "            # print('  - diff_b: ', diff_b)\n",
    "            \n",
    "            # PRINT ALL THE DIFFERENCES\n",
    "            # print('diff_r: ', diff_r)\n",
    "            # print('diff_g: ', diff_g)\n",
    "            # print('diff_b: ', diff_b)\n",
    "            \n",
    "            if diff_r >= COLOR_DIFF_IN_SINGLE_CHANNEL_TRES or diff_g >= COLOR_DIFF_IN_SINGLE_CHANNEL_TRES or \\\n",
    "                diff_b >= COLOR_DIFF_IN_SINGLE_CHANNEL_TRES:\n",
    "                # print('___________________NO GOOD___________________')\n",
    "                good = False\n",
    "            \n",
    "        if good:\n",
    "            final_corners_of_bounding_boxes_after_color_problem.append([ (int(fin_top_left_corner[0]), int(fin_top_left_corner[1])), \n",
    "                                                        (int(fin_bottom_right_corner[0]), int(fin_bottom_right_corner[1]))])\n",
    "            \n",
    "    \n",
    "    return final_corners_of_bounding_boxes_after_color_problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function which is supposed to take a scene image and, provided the dimension of a cell of the accumulator array, \n",
    "#it sections graphically the image in order to visualize the accumulator array referred to the scene\n",
    "\n",
    "def plot_Img_sectioned(img, k1, k2):\n",
    "    H,W = img.shape[:2]\n",
    "    for w in range(0,W,k1):\n",
    "        #Draw a Vertical Line \n",
    "        cv2.line(img,(w,0),(w,H),(0,0,0),3) \n",
    "    for h in range(0,H,k2):\n",
    "        #Draw an Horizontal Line \n",
    "        cv2.line(img,(0,h),(W,h),(0,0,0),3) \n",
    "        \n",
    "    #Plotting the tabled image\n",
    "    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that plots the final merged bounding boxes\n",
    "def plot_final_bounding_boxes(img, final_corners_of_bounding_boxes, difficulty_scenes_images_features, j, thickness=20):\n",
    "    # print('Final corners of bounding boxes:')\n",
    "    # print(final_corners_of_bounding_boxes)\n",
    "    \n",
    "    if img is None:\n",
    "        img = np.copy(difficulty_scenes_images_features[j][IMAGE_INDEX])\n",
    "    #print(img.shape)\n",
    "\n",
    "    \n",
    "    # print final bounding boxes on an image\n",
    "    for top_left_corner, bottom_right_corner in final_corners_of_bounding_boxes:\n",
    "        scene_img_with_FINAL_bounding_boxes = cv2.rectangle(img, top_left_corner, bottom_right_corner, (0,255,0), thickness)\n",
    "\n",
    "    \n",
    "    \n",
    "    # plot image with final bounding boxes (with bounding boxes that do not overlap each other)\n",
    "    if(len(final_corners_of_bounding_boxes) > 0):\n",
    "        # print('  - Found {} instances of model {} in scene {}'.format(len(final_corners_of_bounding_boxes),i,j))\n",
    "        # plotting the bounding box\n",
    "        #plt.imshow(cv2.cvtColor(scene_img_with_FINAL_bounding_boxes, cv2.COLOR_BGR2RGB))\n",
    "        #plt.show()\n",
    "        \n",
    "        return scene_img_with_FINAL_bounding_boxes\n",
    "    else:\n",
    "        # print('  - Model {} NOT found in the scene {}'.format(i,j))\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_result(e_results, idx_of_scene):\n",
    "    for i in models_to_test:\n",
    "        print('Product {} - {} instance/s found:'.format(i, e_results[idx_of_scene][i]['count']))\n",
    "        n = 0\n",
    "        if e_results[idx_of_scene][i].get('width', None):\n",
    "            found_instances = e_results[idx_of_scene][i].get('width')\n",
    "            for index in range(len(found_instances)):\n",
    "                n += 1\n",
    "                print('\\tInstance {} position: {}, width: {}px, height: {}px'.format(n, e_results[idx_of_scene][i]['pos'][index], e_results[idx_of_scene][i]['width'][index], e_results[idx_of_scene][i]['height'][index]))\n",
    "    print('_' * 80 + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "IMAGE_INDEX      = 0\n",
    "KEYPOINT_INDEX   = 1\n",
    "DESCRIPTOR_INDEX = 2\n",
    "\n",
    "models_to_test = [0, 1, 11, 19, 24, 25, 26]\n",
    "scenes_to_test = [1,2,3,4,5]\n",
    "\n",
    "m_results = {}\n",
    "\n",
    "m_final_scene_images_with_bb = {}\n",
    "\n",
    "# Parameters to tune\n",
    "\n",
    "#dimension of a single cell of the accumulator array\n",
    "ACC_ARRAY_CELL_DIMENSION_1 = 120\n",
    "ACC_ARRAY_CELL_DIMENSION_2 = 120\n",
    "# minimum number of votes to consider G as a valid point\n",
    "MIN_VOTES = 1\n",
    "#treshold passed to the Matching function\n",
    "MATCHING_TRESHOLD = 0.45\n",
    "# distance to merge two bounding boxes\n",
    "DISTANCE_BOUNDING_BOXES_TRESHOLD = 200\n",
    "\n",
    "\n",
    "COLOR_DIFF_IN_SINGLE_CHANNEL_TRES = 82\n",
    "\n",
    "N_BINS_ON_WIDTH = 3\n",
    "\n",
    "M_BINS_ON_HEIGHT = 4\n",
    "\n",
    "\n",
    "#print(m_scenes_images_features[1][0].shape[:2])\n",
    "#plt.imshow(cv2.cvtColor(m_scenes_images_features[1][0], cv2.COLOR_BGR2RGB))\n",
    "#plt.show()\n",
    "\n",
    "for j in scenes_to_test:\n",
    "    m_final_scene_images_with_bb[j] = np.copy(m_scenes_images_features[j][IMAGE_INDEX])\n",
    "    m_results[j] = {}\n",
    "    for i in models_to_test:\n",
    "        scene_img = cv2.imread('./scenes/m{}.png'.format(j), cv2.COLOR_BGR2RGB)\n",
    "        model_img = cv2.imread('./models/{}.jpg'.format(i), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # print('_' * 80 + '\\n')\n",
    "        # print('Finding model {} in scene {}'.format(i,j))\n",
    "\n",
    "        m_results[j][i] = {}\n",
    "        m_results[j][i]['count'] = 0\n",
    "        \n",
    "        # Special move\n",
    "        for special_move in [1,2]:\n",
    "            #Finding matches between the keypoints of the scene and the keypoints found in the model\n",
    "            good = Matching(model_images_features[i][DESCRIPTOR_INDEX], m_scenes_images_features[j][DESCRIPTOR_INDEX], \n",
    "                            Treshold=MATCHING_TRESHOLD)\n",
    "\n",
    "        \n",
    "        V = VotingVectors(good,model_images_features, i)\n",
    "\n",
    "        ACC, G_scene, Accumulator_Array_Points = Accumulator_Array(ACC_ARRAY_CELL_DIMENSION_1,\n",
    "                                                                      ACC_ARRAY_CELL_DIMENSION_2,\n",
    "                                                                      scene_img, \n",
    "                                                                      model_img,\n",
    "                                                                      model_images_features, \n",
    "                                                                      m_scenes_images_features, \n",
    "                                                                      good, \n",
    "                                                                      V, \n",
    "                                                                      i, j)\n",
    "        for g in G_scene :\n",
    "            #Drawing a dot in the position of G\n",
    "            #print(g)\n",
    "            #if np.max(ACC)>=20:\n",
    "            cv2.circle(scene_img,(int(g[0]), int(g[1])), 10, (255,55,236),25)\n",
    "\n",
    "        # List of indexes of highlighted cells (list of cells that have a num votes >= MIN_VOTES)\n",
    "        highlighted_cells_of_current_model_in_current_scene = []\n",
    "\n",
    "        # print('List of highlighted cells:')\n",
    "        # print(ACC.shape[0], ACC.shape[1])\n",
    "        for t in range(ACC.shape[0]):\n",
    "            for w in range(ACC.shape[1]):\n",
    "                #print(t,w)\n",
    "                # If a cell of the accumulator array has more than MIN_VOTES than it means that the model has been found\n",
    "                if ACC[t,w] >= MIN_VOTES:\n",
    "\n",
    "\n",
    "                    # print('  - Num votes: {} in cell {} '.format(ACC[t,w], (t,w)))\n",
    "                    # print('Max in Accumulator Array : ', np.max(ACC))\n",
    "                    highlighted_cells_of_current_model_in_current_scene.append([t, w])\n",
    "                    #print(Accumulator_Array_Points[(t, w, 'S')])\n",
    "\n",
    "\n",
    "        # print(highlighted_cells_of_current_model_in_current_scene)\n",
    "                \n",
    "        #V = VotingVectors(good, model_images_features, i)\n",
    "        # estimated centers of the bounding boxes\n",
    "        G_scenes, r = center_estimation(model_images_features, m_scenes_images_features, good, V, j)\n",
    "\n",
    "        # print('r: ', r)\n",
    "        # getting the scaled height and width of the model in the scene\n",
    "        model_height_in_the_scene = model_images_features[i][IMAGE_INDEX].shape[0] / r                                               \n",
    "        model_width_in_the_scene = model_images_features[i][IMAGE_INDEX].shape[1] / r\n",
    "\n",
    "        \n",
    "        # I consider the current bounding box as good only if it has a shape of a rectangle, \n",
    "        # with an height > of the width, as the cereal boxes\n",
    "        if model_width_in_the_scene < model_height_in_the_scene:\n",
    "        \n",
    "            m_scene_img_copy = np.copy(m_scenes_images_features[j][IMAGE_INDEX])\n",
    "\n",
    "            corners_of_bounding_boxes = []\n",
    "\n",
    "            for c in highlighted_cells_of_current_model_in_current_scene :\n",
    "                # Compute G (the center of the cereal box in the scene) as the mean of points that fall \n",
    "                # into the higlighted cell\n",
    "                G_mean = np.mean(Accumulator_Array_Points[(c[0], c[1], 'S')], axis=0)\n",
    "                cv2.circle(scene_img,(int(G_mean[0]), int(G_mean[1])), 20, (0,0,0),25)\n",
    "\n",
    "                cv2.rectangle(scene_img,(c[1]*ACC_ARRAY_CELL_DIMENSION_1,c[0]*ACC_ARRAY_CELL_DIMENSION_2),\n",
    "                              ((c[1]+1)*ACC_ARRAY_CELL_DIMENSION_1,(c[0]+1)*ACC_ARRAY_CELL_DIMENSION_2),(0,255,0),10)\n",
    "\n",
    "\n",
    "\n",
    "                top_left_corner_of_bounding_box = ( int(int(G_mean[0]) - (model_width_in_the_scene / 2) ), \n",
    "                                                int(int(G_mean[1]) - (model_height_in_the_scene / 2) ) )\n",
    "                bottom_right_corner_of_buonding_box = ( int(int(G_mean[0]) + (model_width_in_the_scene / 2) ), \n",
    "                                                int(int(G_mean[1]) + (model_height_in_the_scene / 2) ) )\n",
    "\n",
    "\n",
    "                top_right_corner_of_bounding_box = (bottom_right_corner_of_buonding_box[0], top_left_corner_of_bounding_box[1])\n",
    "\n",
    "                corners_of_bounding_boxes.append([top_left_corner_of_bounding_box, bottom_right_corner_of_buonding_box]) \n",
    "\n",
    "                scene_img_with_bounding_boxes = cv2.rectangle(m_scene_img_copy,\n",
    "                                              top_left_corner_of_bounding_box,\n",
    "                                              bottom_right_corner_of_buonding_box,\n",
    "                                              (0,255,0), 10)\n",
    "\n",
    "\n",
    "\n",
    "            # solve exceeding dimensions of bounding boxes in the scene\n",
    "            final_corners_of_bounding_boxes_without_exceeding_dimensions = solve_exceeding_dimensions_of_bounding_boxes_in_scene(\n",
    "                corners_of_bounding_boxes, m_scenes_images_features)\n",
    "\n",
    "            # solve color problem\n",
    "            #final_corners_of_bounding_boxes_after_color_problem = solve_color_problem(final_corners_of_bounding_boxes_without_exceeding_dimensions, m_scenes_images_features, j)\n",
    "\n",
    "            # solve color problem with bins\n",
    "\n",
    "            final_corners_of_bounding_boxes_after_color_problem = solve_color_problem_with_N_x_M_bins(\n",
    "                final_corners_of_bounding_boxes_without_exceeding_dimensions, \n",
    "                m_scenes_images_features, i, j, N=N_BINS_ON_WIDTH, M=M_BINS_ON_HEIGHT, \n",
    "                COLOR_DIFF_IN_SINGLE_CHANNEL_TRES=COLOR_DIFF_IN_SINGLE_CHANNEL_TRES)\n",
    "\n",
    "            # merge overlapping bounding boxes\n",
    "            final_corners_of_bounding_boxes = merge_overlapping_bounding_boxes(final_corners_of_bounding_boxes_after_color_problem, \n",
    "                                                                               DISTANCE_BOUNDING_BOXES_TRESHOLD)\n",
    "\n",
    "\n",
    "            # print('Model {}'.format(i))\n",
    "            #plotting the model image\n",
    "            #plt.imshow(cv2.cvtColor(model_img, cv2.COLOR_BGR2RGB))\n",
    "            #plt.show()\n",
    "\n",
    "\n",
    "            #plotting the scene image\n",
    "            #plt.imshow(cv2.cvtColor(scene_img, cv2.COLOR_BGR2RGB))\n",
    "            #plt.show()\n",
    "\n",
    "            # plotting every cell of the accumulator array on the scene image\n",
    "            scene_img_sctioned = np.copy(scene_img)\n",
    "            #plot_Img_sectioned(scene_img_sctioned, ACC_ARRAY_CELL_DIMENSION_1, ACC_ARRAY_CELL_DIMENSION_2)\n",
    "            # print(ACC[ACC != 0])\n",
    "\n",
    "            # if I don't find any cell that has the min num of votes, then I don't print the bounding box\n",
    "            #if(len(highlighted_cells_of_current_model_in_current_scene) > 0):\n",
    "                #print('  - Found {} instances of model {} in scene {}'.format(len(highlighted_cells_of_current_model_in_current_scene),i,j))\n",
    "                # plotting the bounding box\n",
    "                #plt.imshow(cv2.cvtColor(scene_img_with_bounding_boxes, cv2.COLOR_BGR2RGB))\n",
    "                #plt.show()\n",
    "            #else:\n",
    "                #print('  - Model {} NOT found in the scene {}'.format(i,j))\n",
    "\n",
    "            # plot image with final bounding boxes (with bounding boxes that do not overlap each other)\n",
    "            #plot_final_bounding_boxes(None, final_corners_of_bounding_boxes_without_exceeding_dimensions, \n",
    "            #                          m_scenes_images_features, j)\n",
    "\n",
    "            #print('AFTER COLOR CORRECTION:')\n",
    "            # plot image with final bounding boxes, after color problem\n",
    "            m_img_bb = plot_final_bounding_boxes(m_final_scene_images_with_bb[j], \n",
    "                                                 final_corners_of_bounding_boxes, \n",
    "                                                 m_scenes_images_features, j)\n",
    "\n",
    "            if m_img_bb is not None:\n",
    "                m_final_scene_images_with_bb[j] = m_img_bb\n",
    "\n",
    "            m_results[j][i]['count'] += len(final_corners_of_bounding_boxes)\n",
    "\n",
    "            for index_1, [top_left_corner, bottom_right_corner] in enumerate(final_corners_of_bounding_boxes):\n",
    "\n",
    "                top_right_corner = (bottom_right_corner[0], top_left_corner[1])\n",
    "\n",
    "                if index_1 == 0:\n",
    "                    m_results[j][i]['width'] = []\n",
    "                    m_results[j][i]['height'] = []\n",
    "                    m_results[j][i]['pos'] = []\n",
    "\n",
    "                # compute width and height of the bounding box\n",
    "                width_of_bounding_box = distance_2_points(top_left_corner, top_right_corner)\n",
    "                height_of_bounding_box = distance_2_points(top_right_corner, bottom_right_corner)\n",
    "\n",
    "\n",
    "                # Save width and height measures of the bounding box\n",
    "                m_results[j][i]['width'].append(width_of_bounding_box)\n",
    "                m_results[j][i]['height'].append(height_of_bounding_box)\n",
    "\n",
    "                # Save the position of the bounding box\n",
    "                m_results[j][i]['pos'].append(\n",
    "                    (int(top_left_corner[0]), int(top_left_corner[1])) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in scenes_to_test:\n",
    "    plt.imshow(cv2.cvtColor(m_final_scene_images_with_bb[j], cv2.COLOR_BGR2RGB))\n",
    "    plt.show()\n",
    "    print_result(m_results, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
